{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from operator import itemgetter\n",
    "import collections\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(99)\n",
    "import tensorflow_federated as tff\n",
    "import numpy as np\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "tf.compat.v1.enable_v2_behavior() # https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_v2_behavior\n",
    "\n",
    "print(f'Tensorflow version: {tf.__version__}')\n",
    "print(f'Tensorflow Federated version: {tff.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "IMG_DATA = input('Image data path: ')\n",
    "IMG_SHAPE = (375, 4)\n",
    "BATCH_SIZE = 32\n",
    "CLASSES = ['aim', 'email', 'facebook', 'ftps', 'gmail', \n",
    "           'hangout', \n",
    "           'icqchat',\n",
    "           'netflix', 'scp', 'sftp',\n",
    "           'skype', 'spotify', 'torrent', 'vimeo', 'voipbuster',\n",
    "           'youtube']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# prepare dataset\n",
    "dataset_root = os.path.abspath(os.path.expanduser(IMG_DATA))\n",
    "print(f'Dataset root: {dataset_root}')\n",
    "\n",
    "img_gen_op = {'classes': CLASSES, 'target_size': IMG_SHAPE, 'batch_size': BATCH_SIZE, 'color_mode': 'grayscale'}\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "def gen_fn(args):\n",
    "    data_path = args.decode('utf-8')\n",
    "    return image_generator.flow_from_directory(data_path,\n",
    "                                               **img_gen_op)\n",
    "\n",
    "dataset_size = dict()\n",
    "queue = [dataset_root]\n",
    "while queue:\n",
    "    path = queue.pop(0)\n",
    "    with os.scandir(path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_dir():\n",
    "                queue.append(entry.path)\n",
    "            if entry.is_file():\n",
    "                name = os.path.basename(os.path.dirname(os.path.dirname(entry.path)))\n",
    "                dataset_size[name] = dataset_size.get(name, 0) + 1\n",
    "\n",
    "dataset_dict = dict()\n",
    "with os.scandir(dataset_root) as it:\n",
    "    for entry in it:\n",
    "        if entry.is_dir():\n",
    "            name = os.path.basename(entry.path)\n",
    "            ds = tf.data.Dataset.from_generator(gen_fn,\n",
    "                                                args=[entry.path],\n",
    "                                                output_types=(tf.float32, tf.float32),\n",
    "                                                output_shapes=(tf.TensorShape((None, ) + IMG_SHAPE + (1, )), \n",
    "                                                               tf.TensorShape([None, len(CLASSES)]))\n",
    "                                               )\n",
    "            dataset_dict[name] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(client_id):\n",
    "    return dataset_dict[client_id]\n",
    "\n",
    "client_data = tff.simulation.ClientData.from_clients_and_fn(\n",
    "                client_ids=list(dataset_dict.keys()),\n",
    "                create_tf_dataset_for_client_fn=client_fn)\n",
    "\n",
    "train_ids = list(dataset_dict.keys())\n",
    "train_ids.remove('0')\n",
    "# train_ids = ['1'] ## for experiment client each\n",
    "dataset = [(client_data.create_tf_dataset_for_client(x), dataset_size[x]) for x in train_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset = (client_data.create_tf_dataset_for_client(client_data.client_ids[0]),\n",
    "                   dataset_size[client_data.client_ids[0]])\n",
    "print(example_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm\n",
    "import statistics\n",
    "# take_value = statistics.median(dataset_size.values())\n",
    "take_value = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset, take_value=None):\n",
    "#     return dataset[0].take(np.ceil(dataset[1]/BATCH_SIZE))\n",
    "    if take_value is None:\n",
    "        take_value = dataset[1]\n",
    "    else:\n",
    "#         take_value = min(take_value, dataset[1])\n",
    "        take_value = 36000\n",
    "    return dataset[0].take(np.ceil(take_value/BATCH_SIZE))\n",
    "    \n",
    "preprocessed_example_dataset = preprocess(example_dataset, take_value)\n",
    "sample_batch = tf.nest.map_structure(lambda x: x.numpy(), iter(preprocessed_example_dataset).next())\n",
    "print(sample_batch[0].shape, sample_batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_dataset = [preprocess(x, take_value) for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation\n",
    "test_dataset = (client_data.create_tf_dataset_for_client('0'), dataset_size['0'])\n",
    "federated_test_data = [preprocess(test_dataset, None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "    # Create Deep and Wide CNN\n",
    "    img_input = tf.keras.Input(shape=IMG_SHAPE+(1, ))\n",
    "    features1 = tf.keras.layers.Conv2D(32, (1, 1), activation='relu')(img_input)\n",
    "    features1 = tf.keras.layers.Flatten()(features1)\n",
    "\n",
    "    features2 = tf.keras.layers.Conv2D(32, (1, 2), activation='relu')(img_input)\n",
    "    features2 = tf.keras.layers.Flatten()(features2)\n",
    "\n",
    "    features3 = tf.keras.layers.Conv2D(32, (1, 4), activation='relu')(img_input)\n",
    "    features3 = tf.keras.layers.Flatten()(features3)\n",
    "\n",
    "    features4 = tf.keras.layers.Conv2D(32, (2, 2), activation='relu')(img_input)\n",
    "    features4 = tf.keras.layers.MaxPooling2D((2, 2), strides=(1, 1))(features4)\n",
    "    features5 = tf.keras.layers.Conv2D(32, (2, 2), activation='relu')(features4)\n",
    "\n",
    "    features4 = tf.keras.layers.Flatten()(features4)\n",
    "    features5 = tf.keras.layers.Flatten()(features5)\n",
    "\n",
    "    x = tf.keras.layers.concatenate([features1, features2, features3, features4, features5])\n",
    "\n",
    "    pred = tf.keras.layers.Dense(len(CLASSES))(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[img_input],\n",
    "                           outputs=[pred])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    keras_model = create_keras_model()\n",
    "    return tff.learning.from_keras_model(keras_model, \n",
    "                                         dummy_batch=sample_batch,\n",
    "                                         loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                                         metrics=[tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_process = tff.learning.build_federated_averaging_process(model_fn,\n",
    "                           client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "                           server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(iterative_process.initialize.type_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = tff.learning.build_federated_evaluation(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROUNDS = 100\n",
    "MAX_STD = 0.001\n",
    "loss = list()\n",
    "accuracy = list()\n",
    "val_loss = list()\n",
    "val_accuracy = list()\n",
    "for round_num in range(1, NUM_ROUNDS+1):\n",
    "    state, metrics = iterative_process.next(state, federated_dataset)\n",
    "    val_metrics = evaluation(state.model, federated_test_data)\n",
    "    loss.append(metrics.loss)\n",
    "    accuracy.append(metrics.categorical_accuracy)\n",
    "    val_loss.append(val_metrics.loss)\n",
    "    val_accuracy.append(val_metrics.categorical_accuracy)\n",
    "    print(f'round: {round_num:2d}, loss: {metrics.loss}, test_accuracy: {val_metrics.categorical_accuracy}')\n",
    "    if len(val_loss) > 3 and np.std(val_loss[-3:]) < MAX_STD:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig1.add_subplot(2, 1, 1)\n",
    "ax1.plot(accuracy, label='Training Accuracy')\n",
    "ax1.plot(val_accuracy, label='Validation Accuracy')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_title('Training and Validation Accuracy')\n",
    "\n",
    "ax2 = fig1.add_subplot(2, 1, 2)\n",
    "ax2.plot(loss, label='Training Loss')\n",
    "ax2.plot(val_loss, label='Validation Loss')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_ylabel('Cross Entropy')\n",
    "ax2.set_ylim([0,max(ax2.get_ylim())])\n",
    "ax2.set_title('Training and Validation Loss')\n",
    "ax2.set_xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('output.pickle', 'wb') as f:\n",
    "    pickle.dump((accuracy, val_accuracy, loss, val_loss), f)\n",
    "with open('output.pickle', 'rb') as f:\n",
    "    print(max(pickle.load(f)[1]))\n",
    "print(max(val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Federated Learning",
   "language": "python",
   "name": "tff-app"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
