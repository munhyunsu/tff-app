{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup library\n",
    "## install -r requirements.txt\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# np.random.seed(99)\n",
    "\n",
    "# jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.random.set_seed(99)\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Global variables\n",
    "NCLIENTS = 16\n",
    "NCLASS = 8\n",
    "IMG_SHAPE = (375, 4)\n",
    "\n",
    "dataset = pd.read_pickle('./electronics/sampled_dataset.pkl')\n",
    "\n",
    "with open('electronics/get_index.pickle', 'rb') as f:\n",
    "    idx2lab, lab2cnt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_reshape(x):\n",
    "    return np.expand_dims(np.array(x).reshape(IMG_SHAPE)/255, axis=-1)\n",
    "dataset['x'] = dataset['x'].apply(pre_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datadiv(nclass=NCLASS):\n",
    "    nlab = len(idx2lab.keys())\n",
    "    \n",
    "    clients = list()\n",
    "    for i in range(0, nlab):\n",
    "        clients.append(list())\n",
    "    \n",
    "    seq = list(range(0, nlab))\n",
    "    random.shuffle(seq)\n",
    "    seq = collections.deque(seq)\n",
    "    for i in range(0, nclass):\n",
    "        seq.rotate(-1)\n",
    "        for j in range(0, len(seq)):\n",
    "            clients[j].append(seq[j])\n",
    "    \n",
    "    return clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fl_dataset(dataset, nclass=NCLASS, val_size=0.1):\n",
    "    dataset = dataset.sample(frac=1)\n",
    "    nlab = len(idx2lab.keys())\n",
    "    nrow = len(dataset)\n",
    "    \n",
    "    flt = dict()\n",
    "    flt_val = pd.Index([])\n",
    "    for i in range(0, nlab):\n",
    "        nrow = len(dataset[dataset['y'] == i])\n",
    "        idx_val = [int(nrow*(1-val_size)), nrow-1]\n",
    "    \n",
    "        idx_trains = list()\n",
    "        nrow = idx_val[0]\n",
    "        tick = nrow//nclass\n",
    "        for j in range(0, nclass):\n",
    "            idx_trains.append(dataset[dataset['y'] == i].index[tick*j:tick*(j+1)])\n",
    "        idx_val[0] = tick*nclass\n",
    "        flt[i] = idx_trains\n",
    "        flt_val = flt_val.union(dataset[dataset['y'] == i].index[idx_val[0]:idx_val[1]])\n",
    "    \n",
    "    datadiv = get_datadiv(nclass)\n",
    "    \n",
    "    train_datasets = list()\n",
    "    for div in datadiv:\n",
    "        train_idx = pd.Index([])\n",
    "        for y in div:\n",
    "            train_idx = train_idx.union(flt[y].pop())\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((dataset['x'][train_idx].values.tolist(),\n",
    "                                                            dataset['y'][train_idx].values.tolist()))\n",
    "        print(f'split train dataset {len(train_idx)}')\n",
    "        train_datasets.append(train_dataset)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((dataset['x'][flt_val].values.tolist(), \n",
    "                                                       dataset['y'][flt_val].values.tolist()))\n",
    "    print(f'split test dataset {len(flt_val)}')\n",
    "    \n",
    "    return train_datasets, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_datasets, raw_test_dataset = create_fl_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 32\n",
    "# SHUFFLE_BUFFER = 100\n",
    "# NUM_EPOCHS = 5\n",
    "# for i in range(0, len(train_datasets)):\n",
    "#     train_datasets[i] = train_datasets[i].repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE)\n",
    "# test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER = 100\n",
    "NUM_EPOCHS = 1\n",
    "def client_fn(client_id):\n",
    "    return raw_train_datasets[client_id].repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE)\n",
    "#     return train_datasets[client_id]\n",
    "\n",
    "client_data = tff.simulation.ClientData.from_clients_and_fn(\n",
    "                client_ids=range(0, len(raw_train_datasets)),\n",
    "                create_tf_dataset_for_client_fn=client_fn)\n",
    "client_data = [client_data.create_tf_dataset_for_client(x) for x in range(0, len(raw_train_datasets))]\n",
    "\n",
    "test_dataset = raw_test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = tf.nest.map_structure(lambda x: x.numpy(), next(iter(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Deep and Wide CNN\n",
    "def create_model(nclass, img_shape=IMG_SHAPE):\n",
    "    img_input = tf.keras.Input(shape=img_shape+(1, ))\n",
    "    features1 = tf.keras.layers.Conv2D(32, (1, 1), activation='relu')(img_input)\n",
    "    features1 = tf.keras.layers.Flatten()(features1)\n",
    "\n",
    "    features2 = tf.keras.layers.Conv2D(32, (1, 2), activation='relu')(img_input)\n",
    "    features2 = tf.keras.layers.Flatten()(features2)\n",
    "\n",
    "    features3 = tf.keras.layers.Conv2D(32, (1, 4), activation='relu')(img_input)\n",
    "    features3 = tf.keras.layers.Flatten()(features3)\n",
    "\n",
    "    features4 = tf.keras.layers.Conv2D(32, (2, 2), activation='relu')(img_input)\n",
    "    features4 = tf.keras.layers.MaxPooling2D((2, 2), strides=(1, 1))(features4)\n",
    "    \n",
    "    features5 = tf.keras.layers.Conv2D(32, (2, 2), activation='relu')(features4)\n",
    "\n",
    "    features4 = tf.keras.layers.Flatten()(features4)\n",
    "    features5 = tf.keras.layers.Flatten()(features5)\n",
    "\n",
    "    x = tf.keras.layers.concatenate([features1, features2, features3, features4, features5])\n",
    "\n",
    "    pred = tf.keras.layers.Dense(nclass)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[img_input],\n",
    "                           outputs=[pred])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckpt(state, metrics, path='./fl_ckpt'):\n",
    "    keras_model = create_model(len(idx2lab.keys()))\n",
    "    keras_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                        metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "    tff.learning.assign_weights_to_keras_model(keras_model, state.model)\n",
    "    keras_model.save(path)\n",
    "    \n",
    "    with open(os.path.join(path, 'fl_metrics.pickle'), 'wb') as f:\n",
    "        pickle.dump(metrics, f)\n",
    "\n",
    "        \n",
    "def load_ckpt(state, path='./fl_ckpt'):\n",
    "    keras_model = tf.keras.models.load_model(path)\n",
    "    state = tff.learning.state_with_new_model_weights(\n",
    "              state,\n",
    "              trainable_weights=[v.numpy() for v in keras_model.trainable_weights],\n",
    "              non_trainable_weights=[v.numpy() for v in keras_model.non_trainable_weights])\n",
    "\n",
    "    with open(os.path.join(path, 'fl_metrics.pickle'), 'rb') as f:\n",
    "        metrics = pickle.load(f)\n",
    "    \n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    # We _must_ create a new model here, and _not_ capture it from an external\n",
    "    # scope. TFF will call this within different graph contexts.\n",
    "    keras_model = create_model(len(idx2lab.keys()))\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=test_dataset.element_spec,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_process = tff.learning.build_federated_averaging_process(\n",
    "        model_fn,\n",
    "        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(iterative_process.initialize.type_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = tff.learning.build_federated_evaluation(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_PATH = f'./ckpt/fl'\n",
    "NUM_ROUNDS = 100\n",
    "start_round = 0\n",
    "loss = list()\n",
    "accuracy = list()\n",
    "val_loss = list()\n",
    "val_accuracy = list()\n",
    "stime = time.time()\n",
    "\n",
    "if os.path.exists(CKPT_PATH):\n",
    "    state, metrics = load_ckpt(state, CKPT_PATH)\n",
    "    start_round = metrics[0]\n",
    "    loss = metrics[1]\n",
    "    accuracy = metrics[2]\n",
    "    val_loss = metrics[3]\n",
    "    val_accuracy = metrics[4]\n",
    "\n",
    "\n",
    "for round_num in range(start_round+1, NUM_ROUNDS+1):\n",
    "#     state, metrics = iterative_process.next(state, train_datasets)\n",
    "    state, metrics = iterative_process.next(state, client_data)\n",
    "    save_ckpt(state, [round_num, loss, accuracy, val_loss, val_accuracy], CKPT_PATH)\n",
    "    val_metrics = evaluation(state.model, [test_dataset])\n",
    "    loss.append(metrics['train']['loss'])\n",
    "    accuracy.append(metrics['train']['sparse_categorical_accuracy'])\n",
    "    val_loss.append(val_metrics['loss'])\n",
    "    val_accuracy.append(val_metrics['sparse_categorical_accuracy'])\n",
    "    print((f'[{int(time.time()-stime)}] round: {round_num:2d}, '\n",
    "           f'metrics: {metrics[\"train\"]}, '\n",
    "           f'val_metrics: {val_metrics}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw learning curves chart\n",
    "acc = accuracy\n",
    "val_acc = val_accuracy\n",
    "loss = loss\n",
    "val_loss = val_loss\n",
    "\n",
    "fig2 = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig2.add_subplot(2, 1, 1)\n",
    "ax1.plot(acc, label='Training Accuracy')\n",
    "ax1.plot(val_acc, label='Validation Accuracy')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_title('Training and Validation Accuracy')\n",
    "\n",
    "ax2 = fig2.add_subplot(2, 1, 2)\n",
    "ax2.plot(loss, label='Training Loss')\n",
    "ax2.plot(val_loss, label='Validation Loss')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_ylabel('Cross Entropy')\n",
    "ax2.set_ylim([0, max(ax2.get_ylim())])\n",
    "ax2.set_title('Training and Validation Loss')\n",
    "ax2.set_xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow Federated",
   "language": "python",
   "name": "tff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
