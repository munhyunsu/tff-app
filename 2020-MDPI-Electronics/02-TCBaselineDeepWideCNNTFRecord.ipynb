{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup library\n",
    "## install -r requirements.txt\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(99)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Global variables\n",
    "IMG_SHAPE = (375, 4)\n",
    "\n",
    "path_pkl = 'sampled_dataset.pickle'\n",
    "dataset = pd.read_pickle(path_pkl)\n",
    "\n",
    "path_index = 'get_index.pickle'\n",
    "with open(path_index, 'rb') as f:\n",
    "    idx2lab, lab2cnt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_reshape(x):\n",
    "    return np.expand_dims(np.array(x).reshape(IMG_SHAPE)/255, axis=-1)\n",
    "dataset['x'] = dataset['vector'].apply(pre_reshape)\n",
    "dataset['y'] = dataset['idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dataset['x'], dataset['y'],\n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train.values.tolist(), y_train.values.tolist()))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test.values.tolist(), y_test.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labset = set()\n",
    "\n",
    "fig = plt.figure(figsize=(8*2, 4))\n",
    "fig.set_facecolor('white')\n",
    "nimg = len(idx2lab.keys())\n",
    "cnt = 0\n",
    "labs = sorted(lab2cnt.keys())\n",
    "for i in range(0, len(x_train)):\n",
    "    vector = np.array(x_train[x_train.index[i]]).reshape(-1)\n",
    "    if sum(vector == 0) > 1500-4*25:\n",
    "        continue\n",
    "    lab = idx2lab[y_train[y_train.index[i]]]\n",
    "    if lab != labs[cnt]:\n",
    "        continue\n",
    "    ax = fig.add_subplot(1, nimg, cnt+1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.grid(False)\n",
    "    ax.imshow(vector[:4*25].reshape((-1, 4)), cmap='gray')\n",
    "    ax.set_xlabel(lab, color='black', fontsize='large')\n",
    "    cnt = cnt + 1\n",
    "    if cnt >= nimg:\n",
    "        break\n",
    "fig.savefig('packet_vector.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Deep and Wide CNN\n",
    "def create_model(nclass, img_shape=IMG_SHAPE):\n",
    "    img_input = tf.keras.Input(shape=img_shape+(1, ))\n",
    "    features1 = tf.keras.layers.Conv2D(32, (1, 1), activation='relu')(img_input)\n",
    "    features1 = tf.keras.layers.Flatten()(features1)\n",
    "\n",
    "    features2 = tf.keras.layers.Conv2D(32, (1, 2), activation='relu')(img_input)\n",
    "    features2 = tf.keras.layers.Flatten()(features2)\n",
    "\n",
    "    features3 = tf.keras.layers.Conv2D(32, (1, 4), activation='relu')(img_input)\n",
    "    features3 = tf.keras.layers.Flatten()(features3)\n",
    "\n",
    "    features4 = tf.keras.layers.Conv2D(32, (2, 2), activation='relu')(img_input)\n",
    "    features4 = tf.keras.layers.MaxPooling2D((2, 2), strides=(1, 1))(features4)\n",
    "    \n",
    "    features5 = tf.keras.layers.Conv2D(32, (2, 2), activation='relu')(features4)\n",
    "\n",
    "    features4 = tf.keras.layers.Flatten()(features4)\n",
    "    features5 = tf.keras.layers.Flatten()(features5)\n",
    "\n",
    "    x = tf.keras.layers.concatenate([features1, features2, features3, features4, features5])\n",
    "\n",
    "    pred = tf.keras.layers.Dense(nclass)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[img_input],\n",
    "                           outputs=[pred])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Log class\n",
    "### https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback\n",
    "class CollectBatchStats(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.batch_losses = []\n",
    "        self.batch_val_losses = []\n",
    "        self.batch_acc = []\n",
    "        self.batch_val_acc = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.batch_losses.append(logs['loss'])\n",
    "        self.batch_acc.append(logs['accuracy'])\n",
    "        self.batch_val_losses.append(logs['val_loss'])\n",
    "        self.batch_val_acc.append(logs['val_accuracy'])\n",
    "        self.model.reset_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(len(idx2lab))\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train build\n",
    "## Compile model for train\n",
    "### tf.keras.optimizers.Adam() default: 0.001 \n",
    "### tf.keras.optimizers.SGD() default: 0.01\n",
    "base_learning_rate = 0.01 \n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 10\n",
    "batch_stats_callback = CollectBatchStats()\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=3)\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epoch,\n",
    "                    validation_data=test_dataset,\n",
    "#                     callbacks=[batch_stats_callback, earlystop_callback])\n",
    "                    callbacks=[batch_stats_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw learning curves chart\n",
    "acc = batch_stats_callback.batch_acc\n",
    "val_acc = batch_stats_callback.batch_val_acc\n",
    "loss = batch_stats_callback.batch_losses\n",
    "val_loss = batch_stats_callback.batch_val_losses\n",
    "\n",
    "fig2 = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig2.add_subplot(2, 1, 1)\n",
    "ax1.plot(acc, label='Training Accuracy')\n",
    "ax1.plot(val_acc, label='Validation Accuracy')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_title('Training and Validation Accuracy')\n",
    "\n",
    "ax2 = fig2.add_subplot(2, 1, 2)\n",
    "ax2.plot(loss, label='Training Loss')\n",
    "ax2.plot(val_loss, label='Validation Loss')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_ylabel('Cross Entropy')\n",
    "ax2.set_ylim([0, max(ax2.get_ylim())])\n",
    "ax2.set_title('Training and Validation Loss')\n",
    "ax2.set_xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('centralized_history.pickle', 'wb') as f:\n",
    "    pickle.dump((acc, val_acc, loss, val_loss), f)\n",
    "print(max(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('centralized_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list()\n",
    "for i in range(0, len(idx2lab)):\n",
    "    class_names.append(idx2lab[i])\n",
    "    \n",
    "predicted_batch = model.predict(test_dataset)\n",
    "predicted_id = np.argmax(predicted_batch, axis=-1)\n",
    "\n",
    "label_id = y_test.values.astype('int64')\n",
    "\n",
    "con_mat = tf.math.confusion_matrix(label_id, predicted_id)\n",
    "\n",
    "result_df = pd.DataFrame(con_mat.numpy(), index=class_names, columns=class_names, dtype=int)\n",
    "\n",
    "print('-- Validation result (Row: Actual Class, Column: Predicted Class) --')\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = np.diag(result_df)\n",
    "\n",
    "true_negative = list()\n",
    "for i in result_df.index:\n",
    "    t = result_df.drop(i, axis=1)\n",
    "    t = t.drop(i, axis=0)\n",
    "    true_negative.append(np.sum(np.sum(t)))\n",
    "true_negative = np.asarray(true_negative)\n",
    "\n",
    "false_positive = np.sum(result_df, axis=1) - true_positive\n",
    "\n",
    "false_negative = np.sum(result_df, axis=0) - true_positive\n",
    "\n",
    "print(f'True positive: {true_positive}')\n",
    "print(f'True negative: {true_negative}')\n",
    "print(f'False positive: {false_positive}')\n",
    "print(f'False negative: {false_negative}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw intermediate image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [model.layers[i].output for i in (1, 3, 4, 5, 6)]\n",
    "intermediate_model = tf.keras.models.Model(inputs=model.input,\n",
    "                                          outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in ['youtube', ]:\n",
    "    ptr = -1\n",
    "    while True:\n",
    "        ptr = ptr + 1\n",
    "        if idx2lab[y_test[y_test.index[ptr]][0]] != target:\n",
    "            continue\n",
    "        break\n",
    "    intermediate_output = intermediate_model.predict(tf.expand_dims(x_test[x_test.index[ptr]], 0))\n",
    "    for idx in range(0, 5):\n",
    "        data = intermediate_output[idx]\n",
    "        fig = plt.figure(figsize=(32, math.ceil(data.shape[-1]/32)*4))\n",
    "        fig.set_facecolor('white')\n",
    "        for i in range(0, 32):\n",
    "            ax = fig.add_subplot(math.ceil(data.shape[-1]/32), 32, i+1)\n",
    "            ax.imshow(data[0, :10, :, i], cmap='binary')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list()\n",
    "for i, l in sorted(idx2lab.items()):\n",
    "    print(i, l)\n",
    "    classes.append(l)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch = model.predict(test_dataset)\n",
    "predicted_id = np.argmax(predicted_batch, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_mat = tf.math.confusion_matrix(np.squeeze(y_test.to_list()), predicted_id)\n",
    "con_mat = con_mat.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "con_mat_df = pd.DataFrame(con_mat_norm, index = classes, columns = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plot_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot()\n",
    "im, cbar = plot_helper.heatmap(con_mat_df, cmap=plt.cm.Reds, row_labels=classes, col_labels=classes, ax=ax)\n",
    "im.set_clim(0, 1)\n",
    "texts = plot_helper.annotate_heatmap(im)\n",
    "fig.savefig('confusion_matrix.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes2 = classes\n",
    "classes = list()\n",
    "reorder = [7, 0, 14, 8, 11, 2, 15, 10, 4, 13, 5, 12, 1, 3, 9, 6]\n",
    "for i in reorder:\n",
    "    classes.append(classes2[i])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_id = list()\n",
    "for p in np.squeeze(y_test.to_list()):\n",
    "    true_id.append(reorder.index(p))\n",
    "true_id = np.array(true_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(data, row_labels, col_labels, ax=None,\n",
    "            cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (N, M).\n",
    "    row_labels\n",
    "        A list or array of length N with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length M with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to `imshow`.\n",
    "    \"\"\"\n",
    "\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(data.shape[1]))\n",
    "    ax.set_yticks(np.arange(data.shape[0]))\n",
    "    # ... and label them with the respective list entries.\n",
    "    ax.set_xticklabels(col_labels)\n",
    "    ax.set_yticklabels(row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    for edge, spine in ax.spines.items():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im, cbar\n",
    "\n",
    "\n",
    "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
    "                     textcolors=[\"black\", \"white\"],\n",
    "                     threshold=None, **textkw):\n",
    "    \"\"\"\n",
    "    A function to annotate a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im\n",
    "        The AxesImage to be labeled.\n",
    "    data\n",
    "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A list or array of two color specifications.  The first is used for\n",
    "        values below a threshold, the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "        separation.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to each call to `text` used to create\n",
    "        the text labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max())/2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\",\n",
    "              verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow Federated",
   "language": "python",
   "name": "tff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
