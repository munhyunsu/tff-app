{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup library\n",
    "## install -r requirements.txt\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(99)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Global variables\n",
    "IMG_SHAPE = (375, 4)\n",
    "\n",
    "path_pkl = 'sampled_dataset.pickle'\n",
    "dataset = pd.read_pickle(path_pkl)\n",
    "\n",
    "path_index = 'get_index.pickle'\n",
    "with open(path_index, 'rb') as f:\n",
    "    idx2lab, lab2cnt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_reshape(x):\n",
    "    return np.expand_dims(np.array(x).reshape(IMG_SHAPE)/255, axis=-1)\n",
    "dataset['x'] = dataset['vector'].apply(pre_reshape)\n",
    "dataset['y'] = dataset['idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dataset['x'], dataset['y'],\n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train.values.tolist(), y_train.values.tolist()))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test.values.tolist(), y_test.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labset = set()\n",
    "\n",
    "fig = plt.figure(figsize=(8*2, 4))\n",
    "fig.set_facecolor('white')\n",
    "nimg = len(idx2lab.keys())\n",
    "cnt = 0\n",
    "labs = sorted(lab2cnt.keys())\n",
    "for i in range(0, len(x_train)):\n",
    "    vector = np.array(x_train[x_train.index[i]]).reshape(-1)\n",
    "    if sum(vector == 0) > 1500-4*25:\n",
    "        continue\n",
    "    lab = idx2lab[y_train[y_train.index[i]]]\n",
    "    if lab != labs[cnt]:\n",
    "        continue\n",
    "    ax = fig.add_subplot(1, nimg, cnt+1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.grid(False)\n",
    "    ax.imshow(vector[:4*25].reshape((-1, 4)), cmap='gray')\n",
    "    ax.set_xlabel(lab, color='black', fontsize='large')\n",
    "    cnt = cnt + 1\n",
    "    if cnt >= nimg:\n",
    "        break\n",
    "fig.savefig('packet_vector.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Deep and Wide CNN\n",
    "def create_model(nclass, img_shape=IMG_SHAPE):\n",
    "    img_input = tf.keras.Input(shape=img_shape+(1, ))\n",
    "    features1 = tf.keras.layers.Conv2D(32, (1, 1), activation='relu')(img_input)\n",
    "    features1 = tf.keras.layers.Flatten()(features1)\n",
    "\n",
    "    features2 = tf.keras.layers.Conv2D(32, (1, 2), activation='relu')(img_input)\n",
    "    features2 = tf.keras.layers.Flatten()(features2)\n",
    "\n",
    "    features3 = tf.keras.layers.Conv2D(32, (1, 4), activation='relu')(img_input)\n",
    "    features3 = tf.keras.layers.Flatten()(features3)\n",
    "\n",
    "    features4 = tf.keras.layers.Conv2D(32, (2, 2), activation='relu')(img_input)\n",
    "    features4 = tf.keras.layers.MaxPooling2D((2, 2), strides=(1, 1))(features4)\n",
    "    \n",
    "    features5 = tf.keras.layers.Conv2D(32, (2, 2), activation='relu')(features4)\n",
    "\n",
    "    features4 = tf.keras.layers.Flatten()(features4)\n",
    "    features5 = tf.keras.layers.Flatten()(features5)\n",
    "\n",
    "    x = tf.keras.layers.concatenate([features1, features2, features3, features4, features5])\n",
    "\n",
    "    pred = tf.keras.layers.Dense(nclass)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[img_input],\n",
    "                           outputs=[pred])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Log class\n",
    "### https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback\n",
    "class CollectBatchStats(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.batch_losses = []\n",
    "        self.batch_val_losses = []\n",
    "        self.batch_acc = []\n",
    "        self.batch_val_acc = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.batch_losses.append(logs['loss'])\n",
    "        self.batch_acc.append(logs['accuracy'])\n",
    "        self.batch_val_losses.append(logs['val_loss'])\n",
    "        self.batch_val_acc.append(logs['val_accuracy'])\n",
    "        self.model.reset_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(len(idx2lab))\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train build\n",
    "## Compile model for train\n",
    "### tf.keras.optimizers.Adam() default: 0.001 \n",
    "### tf.keras.optimizers.SGD() default: 0.01\n",
    "base_learning_rate = 0.01 \n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 10\n",
    "batch_stats_callback = CollectBatchStats()\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=3)\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epoch,\n",
    "                    validation_data=test_dataset,\n",
    "#                     callbacks=[batch_stats_callback, earlystop_callback])\n",
    "                    callbacks=[batch_stats_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw learning curves chart\n",
    "acc = batch_stats_callback.batch_acc\n",
    "val_acc = batch_stats_callback.batch_val_acc\n",
    "loss = batch_stats_callback.batch_losses\n",
    "val_loss = batch_stats_callback.batch_val_losses\n",
    "\n",
    "fig2 = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig2.add_subplot(2, 1, 1)\n",
    "ax1.plot(acc, label='Training Accuracy')\n",
    "ax1.plot(val_acc, label='Validation Accuracy')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_title('Training and Validation Accuracy')\n",
    "\n",
    "ax2 = fig2.add_subplot(2, 1, 2)\n",
    "ax2.plot(loss, label='Training Loss')\n",
    "ax2.plot(val_loss, label='Validation Loss')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_ylabel('Cross Entropy')\n",
    "ax2.set_ylim([0, max(ax2.get_ylim())])\n",
    "ax2.set_title('Training and Validation Loss')\n",
    "ax2.set_xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('centralized_history.pickle', 'wb') as f:\n",
    "    pickle.dump((acc, val_acc, loss, val_loss), f)\n",
    "print(max(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('centralized_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = list()\n",
    "for i in range(0, len(idx2lab)):\n",
    "    class_names.append(idx2lab[i])\n",
    "    \n",
    "predicted_batch = model.predict(test_dataset)\n",
    "predicted_id = np.argmax(predicted_batch, axis=-1)\n",
    "\n",
    "label_id = y_test.values.astype('int64')\n",
    "\n",
    "con_mat = tf.math.confusion_matrix(label_id, predicted_id)\n",
    "\n",
    "result_df = pd.DataFrame(con_mat.numpy(), index=class_names, columns=class_names, dtype=int)\n",
    "\n",
    "print('-- Validation result (Row: Actual Class, Column: Predicted Class) --')\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = np.diag(result_df)\n",
    "\n",
    "true_negative = list()\n",
    "for i in result_df.index:\n",
    "    t = result_df.drop(i, axis=1)\n",
    "    t = t.drop(i, axis=0)\n",
    "    true_negative.append(np.sum(np.sum(t)))\n",
    "true_negative = np.asarray(true_negative)\n",
    "\n",
    "false_positive = np.sum(result_df, axis=1) - true_positive\n",
    "\n",
    "false_negative = np.sum(result_df, axis=0) - true_positive\n",
    "\n",
    "print(f'True positive: {true_positive}')\n",
    "print(f'True negative: {true_negative}')\n",
    "print(f'False positive: {false_positive}')\n",
    "print(f'False negative: {false_negative}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw intermediate image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [model.layers[i].output for i in (1, 3, 4, 5, 6)]\n",
    "intermediate_model = tf.keras.models.Model(inputs=model.input,\n",
    "                                          outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in ['youtube', ]:\n",
    "    ptr = -1\n",
    "    while True:\n",
    "        ptr = ptr + 1\n",
    "        if idx2lab[y_test[y_test.index[ptr]][0]] != target:\n",
    "            continue\n",
    "        break\n",
    "    intermediate_output = intermediate_model.predict(tf.expand_dims(x_test[x_test.index[ptr]], 0))\n",
    "    for idx in range(0, 5):\n",
    "        data = intermediate_output[idx]\n",
    "        fig = plt.figure(figsize=(32, math.ceil(data.shape[-1]/32)*4))\n",
    "        fig.set_facecolor('white')\n",
    "        for i in range(0, 32):\n",
    "            ax = fig.add_subplot(math.ceil(data.shape[-1]/32), 32, i+1)\n",
    "            ax.imshow(data[0, :10, :, i], cmap='binary')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list()\n",
    "for i, l in sorted(idx2lab.items()):\n",
    "    print(i, l)\n",
    "    classes.append(l)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_batch = model.predict(test_dataset)\n",
    "predicted_id = np.argmax(predicted_batch, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_mat = tf.math.confusion_matrix(np.squeeze(y_test.to_list()), predicted_id)\n",
    "con_mat = con_mat.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "con_mat_df = pd.DataFrame(con_mat_norm, index = classes, columns = classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow Federated",
   "language": "python",
   "name": "tff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
