{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from operator import itemgetter\n",
    "import collections\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(99)\n",
    "import tensorflow_federated as tff\n",
    "import numpy as np\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "# tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "print(f'Tensorflow version: {tf.__version__}')\n",
    "print(f'Tensorflow Federated version: {tff.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "# Setup scripts (or notebook)\n",
    "IMG_DATA = './dataset_tma/noniid_15'\n",
    "IMG_SHAPE = (39, 39)\n",
    "BATCH_SIZE = 32\n",
    "# CLASSES = ['aim', 'email', 'facebook', 'ftps', 'gmail', \n",
    "#            'hangout', 'icqchat', 'netflix', 'scp', 'sftp',\n",
    "#            'skype', 'spotify', 'torrent', 'vimeo', 'voipbuster',\n",
    "#            'youtube']\n",
    "CLASSES = ['aim', 'email', 'facebook', 'ftps', 'gmail', \n",
    "           'hangout', 'netflix', 'scp', 'sftp', 'skype',\n",
    "           'spotify', 'torrent', 'vimeo', 'voipbuster', 'youtube']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# prepare dataset\n",
    "dataset_root = os.path.abspath(os.path.expanduser(IMG_DATA))\n",
    "print(f'Dataset root: {dataset_root}')\n",
    "\n",
    "img_gen_op = {'classes': CLASSES, 'target_size': IMG_SHAPE, 'batch_size': BATCH_SIZE}\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "def gen_fn(args):\n",
    "    data_path = args.decode('utf-8')\n",
    "    return image_generator.flow_from_directory(data_path,\n",
    "                                               **img_gen_op)\n",
    "\n",
    "dataset_size = dict()\n",
    "queue = [dataset_root]\n",
    "while queue:\n",
    "    path = queue.pop(0)\n",
    "    with os.scandir(path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_dir():\n",
    "                queue.append(entry.path)\n",
    "            if entry.is_file():\n",
    "                name = os.path.basename(os.path.dirname(os.path.dirname(entry.path)))\n",
    "                dataset_size[name] = dataset_size.get(name, 0) + 1\n",
    "\n",
    "dataset_dict = dict()\n",
    "with os.scandir(dataset_root) as it:\n",
    "    for entry in it:\n",
    "        if entry.is_dir():\n",
    "            name = os.path.basename(entry.path)\n",
    "            ds = tf.data.Dataset.from_generator(gen_fn,\n",
    "                                                args=[entry.path],\n",
    "                                                output_types=(tf.float32, tf.float32),\n",
    "                                                output_shapes=(tf.TensorShape([None, 39, 39, 3]), \n",
    "                                                               tf.TensorShape([None, len(CLASSES)]))\n",
    "                                               )\n",
    "            dataset_dict[name] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(client_id):\n",
    "    return dataset_dict[client_id]\n",
    "\n",
    "client_data = tff.simulation.ClientData.from_clients_and_fn(\n",
    "                client_ids=list(dataset_dict.keys()),\n",
    "                create_tf_dataset_for_client_fn=client_fn)\n",
    "\n",
    "train_ids = list(dataset_dict.keys())\n",
    "train_ids.remove('0')\n",
    "# train_ids = ['1'] ## for experiment client each\n",
    "dataset = [(client_data.create_tf_dataset_for_client(x), dataset_size[x]) for x in train_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset = (client_data.create_tf_dataset_for_client(client_data.client_ids[0]),\n",
    "                   dataset_size[client_data.client_ids[0]])\n",
    "print(example_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm\n",
    "import statistics\n",
    "take_value = statistics.median(dataset_size.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset, take_value=None):\n",
    "#     return dataset[0].take(np.ceil(dataset[1]/BATCH_SIZE))\n",
    "    if take_value is None:\n",
    "        take_value = dataset[1]\n",
    "    else:\n",
    "        take_value = min(take_value, dataset[1])\n",
    "    return dataset[0].take(np.ceil(take_value/BATCH_SIZE))\n",
    "    \n",
    "preprocessed_example_dataset = preprocess(example_dataset, take_value)\n",
    "sample_batch = tf.nest.map_structure(lambda x: x.numpy(), iter(preprocessed_example_dataset).next())\n",
    "print(sample_batch[0].shape, sample_batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_dataset = [preprocess(x, take_value) for x in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation\n",
    "test_dataset = (client_data.create_tf_dataset_for_client('0'), dataset_size['0'])\n",
    "federated_test_data = [preprocess(test_dataset, take_value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compiled_keras_model():\n",
    "    base_learning_rate = 0.001 # default\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "                    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=IMG_SHAPE + (3, )),\n",
    "                    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "                    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "                    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "                    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "                    tf.keras.layers.Flatten(),\n",
    "                    tf.keras.layers.Dense(64, activation='relu'),\n",
    "                    tf.keras.layers.Dense(len(CLASSES))])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    keras_model = create_compiled_keras_model()\n",
    "    return tff.learning.from_compiled_keras_model(keras_model, sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_learning_rate = 0.01 # default: 0.01\n",
    "server_optimizer_fn = lambda : tf.keras.optimizers.SGD(learning_rate=server_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client_weight_fn = lambda reports : reports\n",
    "# OrderedDict([('categorical_accuracy', [<tf.Tensor 'StatefulPartitionedCall:0' shape=() dtype=float32>, \n",
    "#                                        <tf.Tensor 'StatefulPartitionedCall:1' shape=() dtype=float32>]), \n",
    "#              ('loss', [<tf.Tensor 'StatefulPartitionedCall:3' shape=() dtype=float32>, \n",
    "#                        <tf.Tensor 'StatefulPartitionedCall:4' shape=() dtype=float32>]), \n",
    "#              ('keras_training_time_client_sum_sec', \n",
    "#               [<tf.Tensor 'StatefulPartitionedCall:2' shape=() dtype=float32>])])\n",
    "def client_weight_fn(report):\n",
    "    return tf.constant(1.)\n",
    "#     return tf.subtract(1., report['loss'][0])\n",
    "    \n",
    "#     if report['categorical_accuracy'][0] > 0.5:\n",
    "#         return tf.constant(1.)\n",
    "#     else:\n",
    "#         return tf.constant(2.)\n",
    "#     return tf.case([(tf.less(report['categorical_accuracy'], 0.5), lambda: tf.constant(1.))], \n",
    "#                      default=lambda: tf.constant(2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stateless_federated_mean = tff.utils.StatefulAggregateFn(\n",
    "#     initialize_fn=lambda: (),  # The state is an empty tuple.\n",
    "#     next_fn=lambda state, value, weight=None: (\n",
    "#         state, tff.federated_mean(value, weight=weight)))\n",
    "stateless_federated_mean = tff.utils.StatefulAggregateFn(\n",
    "    initialize_fn=lambda: (),  # The state is an empty tuple.\n",
    "    next_fn=lambda state, value, weight=None: (\n",
    "        state, tff.federated_mean(value, weight=weight)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_federated_broadcast = tff.utils.StatefulBroadcastFn(\n",
    "    initialize_fn=lambda: (),\n",
    "    next_fn=lambda state, value: (\n",
    "        state, tff.federated_broadcast(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_process = tff.learning.build_federated_averaging_process(model_fn,\n",
    "#                             server_optimizer_fn=server_optimizer_fn,\n",
    "#                             client_weight_fn=client_weight_fn,\n",
    "#                             stateful_delta_aggregate_fn=stateless_federated_mean,\n",
    "#                             stateful_model_broadcast_fn=stateless_federated_broadcast,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(iterative_process.initialize.type_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = tff.learning.build_federated_evaluation(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROUNDS = 10\n",
    "MAX_STD = 0.001\n",
    "loss = list()\n",
    "accuracy = list()\n",
    "val_loss = list()\n",
    "val_accuracy = list()\n",
    "for round_num in range(1, NUM_ROUNDS+1):\n",
    "    state, metrics = iterative_process.next(state, federated_dataset)\n",
    "    val_metrics = evaluation(state.model, federated_test_data)\n",
    "    loss.append(metrics.loss)\n",
    "    accuracy.append(metrics.categorical_accuracy)\n",
    "    val_loss.append(val_metrics.loss)\n",
    "    val_accuracy.append(val_metrics.categorical_accuracy)\n",
    "    print(f'round: {round_num:2d}, loss: {metrics.loss}, test_accuracy: {val_metrics.categorical_accuracy}')\n",
    "    if len(val_loss) > 3 and np.std(val_loss[-3:]) < MAX_STD:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig1.add_subplot(2, 1, 1)\n",
    "ax1.plot(accuracy, label='Training Accuracy')\n",
    "ax1.plot(val_accuracy, label='Validation Accuracy')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_title('Training and Validation Accuracy')\n",
    "\n",
    "ax2 = fig1.add_subplot(2, 1, 2)\n",
    "ax2.plot(loss, label='Training Loss')\n",
    "ax2.plot(val_loss, label='Validation Loss')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_ylabel('Cross Entropy')\n",
    "ax2.set_ylim([0,max(ax2.get_ylim())])\n",
    "ax2.set_title('Training and Validation Loss')\n",
    "ax2.set_xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('output.pickle', 'wb') as f:\n",
    "    pickle.dump((accuracy, val_accuracy, loss, val_loss), f)\n",
    "with open('output.pickle', 'rb') as f:\n",
    "    print(max(pickle.load(f)[1]))\n",
    "print(max(val_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Federated Learning",
   "language": "python",
   "name": "tff-app"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
